# SoluÃ§Ã£o de Dados em Tempo Real â€“ Squad de Pedidos

## ğŸ“Œ Objetivo
Desenhar uma arquitetura de dados em tempo real para apoiar a squad de pedidos na captura e anÃ¡lise dos eventos transacionais gerados pela aplicaÃ§Ã£o de pedidos.

## ğŸ§© VisÃ£o Geral da SoluÃ§Ã£o

A soluÃ§Ã£o proposta contem:
- EmissÃ£o de eventos pela aplicaÃ§Ã£o de pedidos
- IngestÃ£o em tempo real com Azure Event Hub
- Processamento contÃ­nuo com Databricks Structured Streaming
- Armazenamento no Delta Lake em trÃªs camadas (Bronze, Silver e Gold)
- Consumo dos dados via dashboards (Power BI), alertas (Azure Monitor) ou APIs

! [Diagrama da SoluÃ§Ã£o](docs/Raquel_diagrama.pdf)

## ğŸ—ï¸ Arquitetura

### 1. AplicaÃ§Ã£o de Pedidos
ResponsÃ¡vel por emitir eventos de pedidos via API no formato JSON.

### 2. Azure Event Hub
ServiÃ§o de mensageria em tempo real que recebe os eventos da aplicaÃ§Ã£o. Alta escalabilidade e integraÃ§Ã£o nativa com o Azure.

### 3. Azure Databricks â€“ Structured Streaming
LÃª os dados em tempo real do Event Hub, processa e armazena em Delta Lake.

### 4. Delta Lake
- *Bronze*: dados crus, versÃ£o original dos eventos
- *Silver*: dados limpos, com normalizaÃ§Ãµes e enriquecimentos
- *Gold*: dados prontos para consumo analÃ­tico

### 5. Camadas de Consumo
- Power BI (dashboards)
- Azure Monitor (notificaÃ§Ãµes)
- APIs para integraÃ§Ãµes

> âš ï¸ Por se tratar de um case tÃ©cnico sem acesso ao ambiente real do Azure Databricks, a ingestÃ£o foi simulada com leitura de arquivos JSON em um diretÃ³rio, eu criei uma pasta de input na minha mÃ¡quina, salvei alguns arquivos .json com estrutura semelhante ao schema que coloquei no arquivo abaixo e copiei esses arquivos aos poucos para a pasta (testei com cerca de 40 segundos) para simular eventos chegando. Essa abordagem reproduz o comportamento de streaming com base na chegada de arquivos e permite testar a logica de transformaÃ§Ã£o e escrita de dados de forma que seja minimamente parecida com o ambiente real âš ï¸

## ğŸ’» Exemplo de CÃ³digo

[Trecho de CÃ³digo](code/stream_ingestion_pedidos.py)

## ğŸ§© O que meu cÃ³digo precisa fazer ğŸ§©
- *1*: *Conectar ao Event hub* (simulado)
- *2*: *Ler os dados em tempo real* (com Structured Streaming)
- *3*: *Converter os dados de JSON para colunas*
- *4*: *Gravar no delta lake* (camada bronze, dados raw)

## âš™ï¸ Stack de Tecnologias

| Componente            | Tecnologia                | Justificativa |
|-----------------------|---------------------------|----------------|
| IngestÃ£o de Eventos   | Azure Event Hub           | EscalÃ¡vel, nativo do Azure |
| Processamento         | Databricks Structured Streaming (PySpark) | IntegraÃ§Ã£o com Event Hub e Delta Lake |
| Armazenamento         | Delta Lake (Bronze, Silver, Gold) | GovernanÃ§a e versionamento |
| VisualizaÃ§Ã£o/Alertas  | Power BI, Azure Monitor   | Pronto para uso em real time |


> âš ï¸ Ponto de atenÃ§Ã£o: A ingestÃ£o em tempo real utiliza arquivos JSON monitorados por Spark Strutured Streaming. Por padrÃ£o, o Spark apenas processa **novos arquivos** que aparecem nessa pasta monitorada. Se um arquivo for modificado apÃ³s jÃ¡ ter sido processado, ele nÃ£o serÃ¡ processado automaticamente. Esse comportamento segue o padrÃ£o de uso com o checkpoint para garantir consistÃªncia no consumo de dados. âš ï¸

## ğŸ‘©â€ğŸ’» Sobre mim

Case desenvolvido por Raquel Elias Zanatta Banuth para a vaga de Data Analytics Engineer 
